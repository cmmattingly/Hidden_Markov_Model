{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3900,
   "id": "732fb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    data pre-processing.\n",
    "        Define the basic data structures to hold training and test corpora.\n",
    "        Read in training corpus and build word-index and pos_tag-index mappings\n",
    "'''\n",
    "\n",
    "class Corpora():\n",
    "    \"\"\"\n",
    "    The class holding training and test corpora.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        # word to index (0-based integers) mapping\n",
    "        self.word_index = {}\n",
    "        # POS-tag to index (0-based integers) mapping\n",
    "        self.tag_index = {}\n",
    "        # index to POS-tag mapping: the reverse mapping of the above\n",
    "        self.index_tag = {}\n",
    "        # list of sentences, each of which is a list of pairs of integer indices (word_index[w_t], tag_index[tag_t]),\n",
    "        # where w_t and tag_t are the word and POS tag at the location t of a sentence, respectively.\n",
    "        self.training_sentences = []\n",
    "        # list of sentences, each of which is a list of integer indices (word_index[w_t])\n",
    "        self.test_sentences = []\n",
    "\n",
    "        self.max_len = 0\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def read_corpus(self, corpus_path, is_training):\n",
    "        \"\"\"\n",
    "        Read a corpus\n",
    "        :param corpus_path: path to a file with POS-tagged sentences.\n",
    "        :param is_training: if true, the file is for the training corpus, otherwise the test corpus\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(corpus_path, 'r') as f:\n",
    "            # holding the current sentence\n",
    "            cur_sentence = []\n",
    "            self.max_len = 0\n",
    "            while True:\n",
    "                # each line is a (word, POS_tag, other_label) tuple.\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                # sentences are delimited by an empty line.\n",
    "                if len(line) == 1:\n",
    "                    if is_training:\n",
    "                        self.training_sentences.append(cur_sentence)\n",
    "                    else:\n",
    "                        self.test_sentences.append(cur_sentence)\n",
    "                    if len(cur_sentence) > self.max_len:\n",
    "                        self.max_len = len(cur_sentence)\n",
    "                    cur_sentence = []\n",
    "                    continue\n",
    "                w, t, _ = line.strip().split()\n",
    "                if w not in self.word_index:\n",
    "                    self.word_index[w] = len(self.word_index)\n",
    "                if t not in self.tag_index:\n",
    "                    self.tag_index[t] = len(self.tag_index)\n",
    "                    self.index_tag[len(self.tag_index) - 1] = t\n",
    "                cur_sentence.append((self.word_index[w], self.tag_index[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3901,
   "id": "484989ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = Corpora()\n",
    "corpora.read_corpus('./data/train.txt', is_training=True)\n",
    "corpora.read_corpus('./data/test.txt', is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3902,
   "id": "8e66f710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19122, 10),\n",
       " (1112, 10),\n",
       " (938, 10),\n",
       " (30, 13),\n",
       " (14446, 10),\n",
       " (129, 0),\n",
       " (105, 21),\n",
       " (302, 29),\n",
       " (2511, 21),\n",
       " (23, 2),\n",
       " (3562, 8),\n",
       " (459, 0),\n",
       " (19123, 16),\n",
       " (389, 17),\n",
       " (2512, 0),\n",
       " (206, 1),\n",
       " (3758, 10),\n",
       " (163, 10),\n",
       " (7, 6),\n",
       " (1723, 7),\n",
       " (9528, 8),\n",
       " (2248, 9),\n",
       " (15, 1),\n",
       " (3758, 10),\n",
       " (30, 13),\n",
       " (17909, 18),\n",
       " (19124, 9),\n",
       " (33, 14)]"
      ]
     },
     "execution_count": 3902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora.test_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3903,
   "id": "ca60965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    implement the forward, backward, and Viterbi algorithms.\n",
    "    Implementation based on: http://www.cs.sjsu.edu/faculty/stamp/RUA/HMM.pdf\n",
    "'''\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class HMM():\n",
    "    \"\"\"\n",
    "    The HMM class that holds data structures and implementations for the forward, backward, and Viterbi algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpora):\n",
    "        # self.corpora = corpora\n",
    "        self.num_words = len(corpora.word_index)\n",
    "        self.num_tags = len(corpora.tag_index)\n",
    "        self.max_sentence_len = corpora.max_len\n",
    "\n",
    "        # a very small positive number for Laplacian smoothing\n",
    "        self.eps = 1e-8\n",
    "\n",
    "        # for HMM parameters obtained from MLE on the POS-tagged corpus\n",
    "        self.A0 = np.zeros((self.num_tags, self.num_tags)) + self.eps\n",
    "        self.B0 = np.zeros((self.num_tags, self.num_words)) + self.eps\n",
    "        self.pi0 = np.zeros((self.num_tags, 1)) + self.eps\n",
    "\n",
    "        # for HMM parameters estimated iteratively during EM.\n",
    "        self.A = np.zeros((self.num_tags, self.num_tags)) + self.eps\n",
    "        self.B = np.zeros((self.num_tags, self.num_words)) + self.eps\n",
    "        self.pi = np.zeros((self.num_tags, 1)) + self.eps\n",
    "\n",
    "        # for forward algorithm\n",
    "        self.alpha = np.zeros((self.num_tags, self.max_sentence_len))\n",
    "        self.scales = np.zeros((1, self.max_sentence_len))\n",
    "\n",
    "        # for backward algorithm\n",
    "        self.beta = np.zeros((self.num_tags, self.max_sentence_len))\n",
    "\n",
    "        # for Viterbi algorithm\n",
    "        self.v = np.zeros((self.num_tags, self.max_sentence_len)) #viterbi\n",
    "        self.back_pointer = np.zeros((self.num_tags, self.max_sentence_len))\n",
    "        self.pred_seq = np.zeros((1, self.max_sentence_len))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def mle(self, training_sentences):\n",
    "        \"\"\"\n",
    "        Use MLE to initialize the HMM parameters A, B, and pi\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #A0, B0, and PI0 are intialized the same way as A, B, and PI. (just set them equal to eachother)\n",
    "        #For A0, B0, and PI0, we can find these using the relative frequency to estimate all 3. \n",
    "        #Go through all sentences and see how often a tag happen in the first position, then divide that by the number of sentence\n",
    "        \n",
    "        #pi /\n",
    "        #A /\n",
    "        #B /\n",
    "        \n",
    "        #create list of just tags: /\n",
    "        training_pos_tags = []\n",
    "        for sentence in training_sentences:\n",
    "            new_sentence = []\n",
    "            for tpl in sentence:\n",
    "                _ , tag_index = tpl\n",
    "                new_sentence.append(tag_index)\n",
    "            training_pos_tags.append(new_sentence)\n",
    "        #------------------------------------------------------\n",
    "        \n",
    "        #initialize B0: /\n",
    "        for sentence in training_sentences:\n",
    "            for tpl in sentence:\n",
    "                word_index, tag_index = tpl\n",
    "                self.B0[tag_index][word_index] += 1\n",
    "\n",
    "        for i in range(self.num_tags):\n",
    "            self.B0[i] /= np.sum(self.B0[i])\n",
    "            \n",
    "        #-------------------------------------------------------\n",
    "        \n",
    "        #initialize pi0 /\n",
    "        for sentence in training_sentences:\n",
    "            _ , tag_index0 = sentence[0]\n",
    "            self.pi0[tag_index0] += 1\n",
    "        \n",
    "        for i in range(len(self.pi0)):\n",
    "            self.pi0[i] /= len(training_sentences)\n",
    "        #-------------------------------------------------------\n",
    "        \n",
    "        #Initialize A0: /\n",
    "        for sentence in training_pos_tags:\n",
    "            bi_gram = list(zip(sentence, sentence[1:]))\n",
    "            for tpl in bi_gram:\n",
    "                i1, i2 = tpl\n",
    "                self.A0[i1][i2] += 1        \n",
    "        \n",
    "        for i in range(len(self.A0)):\n",
    "            self.A0[i] /= np.sum(self.A0[i])\n",
    "        \n",
    "        #------------------------------------------------------\n",
    "        \n",
    "        self.A = self.A0\n",
    "        self.B = self.B0\n",
    "        self.pi = self.pi0\n",
    "        \n",
    "        # -------------------------------------------------------------------------\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"\n",
    "        Run the forward algorithm on a sentence and populate the alpha array.\n",
    "        :param sentence: a sentence on which the forward algorithm runs. The sentence is typically from the unlabeled set.\n",
    "        :return: log-likelihood, computed according to the project description using the local normalization factors.\n",
    "        \"\"\"\n",
    "        #the probability of a state at a certain time, given the history of evidence, previous alpha\n",
    "        #alpha array = 44 x 70, 70 being the max sentence length (shouldn't it be 70 x 44 to make the calculations easier)?\n",
    "        #state: POS_tags\n",
    "        #observations: words\n",
    "        \n",
    "        #Base Case: t == 0\n",
    "        for i in range(self.num_tags):\n",
    "            self.alpha[i][0] = self.pi[i] * self.B[i][sentence[0][0]]\n",
    "            self.scales[0][0] += self.alpha[i][0]\n",
    "            \n",
    "        #Scale Base Case:\n",
    "        self.scales[0][0] = 1 / self.scales[0][0]\n",
    "        for i in range(self.num_tags):\n",
    "            self.alpha[i][0] = self.scales[0][0] * self.alpha[i][0]\n",
    "            \n",
    "        #Case: t > 0\n",
    "        for t in range(1, len(sentence)): \n",
    "            for i in range(self.num_tags): \n",
    "                for j in range(self.num_tags):\n",
    "                    self.alpha[i][t] = self.alpha[i][t] + (self.alpha[j][t-1] * self.A[j][i])\n",
    "                self.alpha[i][t] = self.alpha[i][t] * self.B[i][sentence[t][0]]\n",
    "                self.scales[0][t] += self.alpha[i][t]\n",
    "            self.scales[0][t] = 1 / self.scales[0][t]\n",
    "            for i in range(self.num_tags):\n",
    "                self.alpha[i][t] = self.scales[0][t] * self.alpha[i][t]\n",
    "                \n",
    "        log_p = 0.0\n",
    "        for t in range(len(sentence)):\n",
    "            log_p += np.log(self.scales[0][t])\n",
    "        log_p = log_p * -1\n",
    "        return log_p\n",
    "                    \n",
    "    # -------------------------------------------------------------------------\n",
    "    def backward(self, sentence):\n",
    "        \"\"\"\n",
    "        Run the backward algorithm on the d-th training sentence and populate the beta array\n",
    "        :param sentence: a sentence on which the forward algorithm runs. The sentence is typically from the unlabeled set.\n",
    "        :return:\n",
    "        \"\"\"                                      \n",
    "        for i in range(self.num_tags):\n",
    "            self.beta[i][len(sentence)-1] = self.scales[0][len(sentence)-1]\n",
    "        \n",
    "        for t in range(len(sentence)-2, -1, -1):\n",
    "            for i in range(self.num_tags):\n",
    "                for j in range(self.num_tags):\n",
    "                    self.beta[i][t] = self.beta[i][t] + ((self.A[i][j] * self.B[j][sentence[t+1][0]]) *\n",
    "                                                        self.beta[j][t+1])\n",
    "                self.beta[i][t] = self.scales[0][t] * self.beta[i][t]\n",
    "                                                                  \n",
    "    # -------------------------------------------------------------------------\n",
    "    def Viterbi(self, sentence):\n",
    "        \"\"\"\n",
    "        Run the Viterbi algorithm on the d-th training sentence.\n",
    "        Populate the v, back_point, and pred_seq arrays.\n",
    "        Note that the v array stores the log of the Viterbi values to avoid underflow.\n",
    "\n",
    "        :param sentence: a sentence on which the forward algorithm runs. The sentence is typically from the unlabeled set.\n",
    "        :return: log Pr(best_Q | O)\n",
    "        \"\"\"\n",
    "        for i in range(self.num_tags):\n",
    "            self.v[i][0] = self.pi[i] * self.B[i][sentence[0][0]]\n",
    "            self.back_pointer[i][0] = 0\n",
    "        \n",
    "        for t in range(1, len(sentence)):\n",
    "            for i in range(self.num_tags):\n",
    "                prob = self.v[:, t-1] * self.A[:, i] * self.B[:, sentence[t][0]]\n",
    "                self.v[i, t] = np.max(prob)\n",
    "                self.back_pointer[i, t] = np.argmax(prob)\n",
    "        bestpathprob = np.max(self.v[:, len(sentence)-1])\n",
    "        bestpathpointer = np.argmax(self.v[:, 0])\n",
    "        \n",
    "        for t in range(0, len(sentence)):\n",
    "            np.log(self.v[:, t])\n",
    "        \n",
    "        t = len(sentence)-1\n",
    "        arr = np.zeros((1, len(sentence)))\n",
    "        self.pred_seq[0][0] = bestpathpointer\n",
    "        while t>0:\n",
    "            self.pred_seq[0][t] = self.back_pointer[bestpathpointer,t]\n",
    "            j = i\n",
    "            t -= 1\n",
    "        np.flip(self.pred_seq)\n",
    "        \n",
    "        return np.log(bestpathprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3904,
   "id": "85a5c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    EM for HMM training\n",
    "        Use functions in problem1.py and problem2.py to run EM algorithm and train/evaluate HMM.\n",
    "'''\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "def em(model, corpora, mu, num_iters = 30):\n",
    "    \"\"\"\n",
    "    The EM algorithm for training HMM.\n",
    "    :param model: the HMM model to be trained\n",
    "    :param corpora: training and test corpora\n",
    "    :param mu: relative importance of MLE\n",
    "    :param num_iters: number of total EM iterations\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize model parameters using the POS-tagged sentences\n",
    "    model.mle(corpora.training_sentences)\n",
    "\n",
    "    for it in range(num_iters):\n",
    "        # reset the ksi and gamma matrices and get ready for accumulation of the soft frequencies\n",
    "\n",
    "        # new_= np.zeros((model.num_tags, 1))\n",
    "        new_A = np.zeros((model.num_tags, model.num_tags))\n",
    "        new_B = np.zeros((model.num_tags, model.num_words))\n",
    "        new_pi = np.zeros((model.num_tags, 1))\n",
    "\n",
    "        # the E-step: go through the unlabeled corpus and update the ksi & gamma matrices\n",
    "        log_likelihood = 0\n",
    "        for i, sentence in enumerate(corpora.test_sentences):\n",
    "            log_likelihood += em_one_sentence(model, sentence, new_A, new_B, new_pi)\n",
    "        print(log_likelihood)\n",
    "        # normalize new_A, new_B, and new_pi\n",
    "        # update HMM parameters A, B, and pi using the new_A, new_B, and new_pi\n",
    "        maximization(model, new_A, new_B, new_pi, mu)\n",
    "\n",
    "        accuracy, log_p = evaluate(model, corpora.test_sentences)\n",
    "        print(accuracy, log_p)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "def em_one_sentence(model, sentence, new_A, new_B, new_pi):\n",
    "    \"\"\"\n",
    "    Run the expectation step using the fixed HMM model on a single sentence.\n",
    "    The soft fraquencies are accumulated into the last four matrices.\n",
    "    :param model: the current fixed HMM\n",
    "    :param sentence: a sentence from test set, presumably unlabeled.\n",
    "    :param new_A: sum of transition soft frequencies (tag_i -> tag_j)\n",
    "    :param new_B: sum of emission soft frequencies (tag_i -> word_o)\n",
    "    :param new_pi: sum of starting soft frequencies (tag_i)\n",
    "    :return: log likelihood obtained from the forward algorithm\n",
    "    \"\"\"\n",
    "#     log_p = 0.0\n",
    "#     for t in range(len(sentence)):\n",
    "#         log_p += np.log(model.scales[0][t])\n",
    "#     log_p = log_p * -1\n",
    "    \n",
    "#     new_A = np.zeros((model.num_tags, model.num_tags))\n",
    "#     new_B = np.zeros((model.num_tags, model.num_words))\n",
    "#     new_pi = np.zeros((model.num_tags, 1))\n",
    "        \n",
    "    #compute gamma and digamma\n",
    "    gamma = np.zeros((model.num_tags, len(sentence)))\n",
    "    digamma = np.zeros((model.num_tags, model.num_tags, len(sentence)))\n",
    "    \n",
    "    for t in range(len(sentence)-1):\n",
    "        denom = 0\n",
    "        for i in range(model.num_tags):\n",
    "            for j in range(model.num_tags):\n",
    "                denom += model.alpha[i][t] * model.A[i][j] * model.B[j][sentence[t+1][0]] * model.beta[j][t+1]\n",
    "        for i in range(model.num_tags):\n",
    "            for j in range(model.num_tags):\n",
    "                digamma[i][j][t] = (model.alpha[i][t] * model.A[i][j] * model.B[j][sentence[t+1][0]] * model.beta[j][t+1]) / denom\n",
    "                gamma[i][t] += digamma[i][j][t]\n",
    "        \n",
    "        #special case for gammaT-1(i)\n",
    "        denom = 0\n",
    "        for i in range(model.num_tags):\n",
    "            denom += model.alpha[i][len(sentence)-1]\n",
    "        for i in range(model.num_tags):\n",
    "            gamma[i][len(sentence)-1] = model.alpha[i][len(sentence)-1] / denom\n",
    "    \n",
    "    #new_pi\n",
    "    for i in range(model.num_tags):\n",
    "        new_pi[i] = gamma[i][0]\n",
    "    \n",
    "    #seem to not get new_A (gamma and digamma seem to be right compared to yours)\n",
    "    #new_A\n",
    "    for i in range(model.num_tags):\n",
    "        for j in range(model.num_tags):\n",
    "            numer = 0\n",
    "            denom = 0\n",
    "            for t in range(len(sentence)-1):\n",
    "                numer += digamma[i][j][t]\n",
    "                denom += gamma[i][t]\n",
    "            new_A[i][j] = numer/denom\n",
    "    \n",
    "    #new_B\n",
    "    for i in range(model.num_tags):\n",
    "        for j in range(model.num_words):\n",
    "            numer = 0\n",
    "            denom = 0\n",
    "            for t in range(len(sentence)):\n",
    "                if(sentence[t][0] == j):\n",
    "                    numer += gamma[i][t]\n",
    "                denom += gamma[i][t]\n",
    "            new_B[i][j] = numer/denom\n",
    "    return model.forward(sentence)\n",
    "            \n",
    "    \n",
    "# -------------------------------------------------------------------------\n",
    "def maximization(model, new_A, new_B, new_pi, mu):\n",
    "    \"\"\"\n",
    "    Update HMM parameters A, B, and pi\n",
    "    :param model:\n",
    "    :param new_A:\n",
    "    :param new_B:\n",
    "    :param new_pi:\n",
    "    :param mu: a real number between [0, 1]. Relative importance of the MLE\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sum_B = np.sum(new_B, axis=1)\n",
    "    for i in range(model.num_tags):\n",
    "        new_B[i]=new_B[i]/sum_B[i]\n",
    "        \n",
    "    model.A = mu * model.A + (1 - mu) * new_A\n",
    "    model.B = mu * model.B + (1-mu) * new_B\n",
    "    model.pi = mu * model.pi + (1-mu) * new_pi\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "def evaluate(model, test_corpus):\n",
    "    \"\"\"\n",
    "    Use Viterbi algorithm to predict POS tags for the sentences in the test_corpus.\n",
    "    :param model: an HMM\n",
    "    :param test_corpus: list of POS-tagged sentences\n",
    "    :return: accuracy of the prediction, defined as the percentage of tags that are predicted correctly.\n",
    "    \"\"\"\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    total_log_p = 0\n",
    "    for sentence in test_corpus:\n",
    "        max_log_p = model.Viterbi(sentence)\n",
    "        total_log_p += max_log_p\n",
    "        for t, (o, q) in enumerate(sentence):\n",
    "            if model.pred_seq[0, t] == q:\n",
    "                correct += 1.0\n",
    "        total += len(sentence)\n",
    "    return correct / total, total_log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3905,
   "id": "ac22ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HMM(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3906,
   "id": "45980e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.mle(corpora.training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3907,
   "id": "3e854bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = model.forward(corpora.test_sentences[0]) #different if you run this cell again, so don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3908,
   "id": "7f6aa7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward(corpora.test_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3909,
   "id": "f3119323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-252.44455993227464\n"
     ]
    }
   ],
   "source": [
    "max_log_p = model.Viterbi(corpora.test_sentences[0])\n",
    "print(max_log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3910,
   "id": "c63e61fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42., 10., 10., 13.,  0.,  0., 21., 29., 21.,  2.,  8.,  0.,  0.,\n",
       "        17.,  0.,  1., 10., 10.,  6.,  7.,  8.,  9.,  1., 10., 13.,  0.,\n",
       "         0., 14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 3910,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pred_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3911,
   "id": "d011b097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248.86971046348873"
      ]
     },
     "execution_count": 3911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_A = np.zeros((model.num_tags, model.num_tags))\n",
    "new_B = np.zeros((model.num_tags, model.num_words))\n",
    "new_pi = np.zeros((model.num_tags, 1))\n",
    "em_one_sentence(model, corpora.test_sentences[0], new_A, new_B, new_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3912,
   "id": "2b43f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximization(model, new_A, new_B, new_pi, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3913,
   "id": "1d8e3244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0119059 , 0.01115603, 0.04845907, 0.00395955])"
      ]
     },
     "execution_count": 3913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.A[:4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3914,
   "id": "dfd3f641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.82313408e-14, 4.49358146e-14, 5.48944554e-14, 6.71832682e-13])"
      ]
     },
     "execution_count": 3914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.B[:4, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3915,
   "id": "378a5d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00483173],\n",
       "       [0.02939046],\n",
       "       [0.04863724],\n",
       "       [0.00072883]])"
      ]
     },
     "execution_count": 3915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pi[:4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
